{"_meta":{"note":"Append one JSON object per line after this entry. Each sample needs `question`, `contexts`, `answer`, and `ground_truth`. Remove or keep this metadata entry out of evaluation runs if desired."}}
{"question":"What is the central contribution of 'Attention Is All You Need'?","user_input":"What is the central contribution of 'Attention Is All You Need'?","contexts":["The Transformer replaces recurrence with self-attention to model long-range dependencies while enabling parallelization.","Multi-head attention and positional encodings allow the model to capture relationships across sequences without RNNs."],"answer":"It introduces the Transformer, a self-attention architecture that drops recurrence and uses multi-head attention with positional encodings for efficient sequence modeling.","response":"It introduces the Transformer, a self-attention architecture that drops recurrence and uses multi-head attention with positional encodings for efficient sequence modeling.","ground_truth":"Introduces the Transformer architecture, using self-attention plus positional encodings to model sequences without recurrence."}
{"question":"How does PaperNet differ from baseline GNNs in Paper.AI's benchmark?","user_input":"How does PaperNet differ from baseline GNNs in Paper.AI's benchmark?","contexts":["PaperNet fuses graph neighborhoods from Neo4j with cross-document attention over retrieved chunks.","Baseline GNNs only propagate along citation edges without conditioning on retrieved text passages."],"answer":"PaperNet augments standard GNN propagation with cross-document attention on the retrieved text chunks, letting it combine citation structure with content.","response":"PaperNet augments standard GNN propagation with cross-document attention on the retrieved text chunks, letting it combine citation structure with content.","ground_truth":"It combines Neo4j neighborhood signals with attention over retrieved text, unlike the baseline GNNs that operate purely on the citation graph."}
{"question":"Which optimization schedule does the Diffusion-XL paper report?","user_input":"Which optimization schedule does the Diffusion-XL paper report?","contexts":["Training uses AdamW with a cosine decay schedule after a 5k step warmup.","We also scale the learning rate with batch size following the linear scaling rule."],"answer":"They fine-tune with AdamW, apply a short warmup, and then follow a cosine decay learning-rate schedule tied to the linear scaling rule.","response":"They fine-tune with AdamW, apply a short warmup, and then follow a cosine decay learning-rate schedule tied to the linear scaling rule.","ground_truth":"AdamW optimizer with ~5k step warmup, followed by cosine decay and linear scaling with batch size."}

